[
  {
    "title": "VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via\n  Deformable 3D Gaussians",
    "authors": [
      "Siyuan Yao",
      "Chaoli Wang"
    ],
    "abstract": "Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.",
    "arxiv_url": "http://arxiv.org/abs/2507.12667v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12667v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "tracking",
      "segmentation",
      "dynamic",
      "ar",
      "deformation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
    "authors": [
      "Davide Di Nucci",
      "Matteo Tomei",
      "Guido Borghi",
      "Luca Ciuffreda",
      "Roberto Vezzani",
      "Rita Cucchiara"
    ],
    "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.",
    "arxiv_url": "http://arxiv.org/abs/2507.12095v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12095v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D\n  Gaussian Splatting Representation",
    "authors": [
      "Beining Xu",
      "Siting Zhu",
      "Hesheng Wang"
    ],
    "abstract": "We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at https://github.com/IRMVLab/SGLoc.",
    "arxiv_url": "http://arxiv.org/abs/2507.12027v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12027v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV",
      "cs.RO",
      "I.4.8; I.2.9"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d gaussian",
      "semantic",
      "ar",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark",
    "authors": [
      "Jingqian Wu",
      "Peiqi Duan",
      "Zongqiang Wang",
      "Changwei Wang",
      "Boxin Shi",
      "Edmund Y. Lam"
    ],
    "abstract": "In low-light environments, conventional cameras often struggle to capture clear multi-view images of objects due to dynamic range limitations and motion blur caused by long exposure. Event cameras, with their high-dynamic range and high-speed properties, have the potential to mitigate these issues. Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction, facilitating bright frame synthesis from multiple viewpoints in low-light conditions. However, naively using an event-assisted 3D GS approach still faced challenges because, in low light, events are noisy, frames lack quality, and the color tone may be inconsistent. To address these issues, we propose Dark-EvGS, the first event-assisted 3D GS framework that enables the reconstruction of bright frames from arbitrary viewpoints along the camera trajectory. Triplet-level supervision is proposed to gain holistic knowledge, granular details, and sharp scene rendering. The color tone matching block is proposed to guarantee the color consistency of the rendered frames. Furthermore, we introduce the first real-captured dataset for the event-guided bright frame synthesis task via 3D GS-based radiance field reconstruction. Experiments demonstrate that our method achieves better results than existing methods, conquering radiance field reconstruction under challenging low-light conditions. The code and sample data are included in the supplementary material.",
    "arxiv_url": "http://arxiv.org/abs/2507.11931v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11931v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Sicheng Yu",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction, which effectively balances rendering quality, efficiency, and speed. However, existing 3DGS approaches usually generate plausible outputs and face significant challenges in complex scene reconstruction, manifesting as incomplete holistic structural outlines and unclear local lighting effects. To address these issues simultaneously, we propose a novel decoupled optimization framework, which integrates wavelet decomposition into 3D Gaussian Splatting and 2D sampling. Technically, through 3D wavelet decomposition, our approach divides point clouds into high-frequency and low-frequency components, enabling targeted optimization for each. The low-frequency component captures global structural outlines and manages the distribution of Gaussians through voxelization. In contrast, the high-frequency component restores intricate geometric and textural details while incorporating a relight module to mitigate lighting artifacts and enhance photorealistic rendering. Additionally, a 2D wavelet decomposition is applied to the training images, simulating radiance variations. This provides critical guidance for high-frequency detail reconstruction, ensuring seamless integration of details with the global structure. Extensive experiments on challenging datasets demonstrate our method achieves state-of-the-art performance across various metrics, surpassing existing approaches and advancing the field of 3D scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2507.12498v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12498v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface\n  Reconstruction",
    "authors": [
      "Haoxuan Qu",
      "Yujun Cai",
      "Hossein Rahmani",
      "Ajay Kumar",
      "Junsong Yuan",
      "Jun Liu"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.",
    "arxiv_url": "http://arxiv.org/abs/2507.11321v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11321v1",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "high quality"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth\n  Reconstruction via Physics Simulation for Scene Update",
    "authors": [
      "Jeongyun Kim",
      "Seunghoon Jeong",
      "Giseop Kim",
      "Myung-Hwan Jeon",
      "Eunji Jun",
      "Ayoung Kim"
    ],
    "abstract": "Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.",
    "arxiv_url": "http://arxiv.org/abs/2507.11069v2",
    "pdf_url": "http://arxiv.org/pdf/2507.11069v2",
    "published_date": "2025-07-15",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "face",
      "understanding",
      "geometry",
      "reflection",
      "dynamic",
      "sparse-view",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling",
    "authors": [
      "Hayeon Kim",
      "Ji Ha Jang",
      "Se Young Chun"
    ],
    "abstract": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing.",
    "arxiv_url": "http://arxiv.org/abs/2507.11061v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11061v1",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "geometry",
      "slam",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions",
    "authors": [
      "Shivangi Aneja",
      "Sebastian Weiss",
      "Irene Baeza",
      "Prashanth Chandran",
      "Gaspard Zoss",
      "Matthias Nießner",
      "Derek Bradley"
    ],
    "abstract": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.",
    "arxiv_url": "http://arxiv.org/abs/2507.10542v1",
    "pdf_url": "http://arxiv.org/pdf/2507.10542v1",
    "published_date": "2025-07-14",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "human",
      "face",
      "avatar",
      "dynamic",
      "ar",
      "head",
      "gaussian splatting",
      "high-fidelity",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for\n  Autonomous Driving",
    "authors": [
      "Yixun Zhang",
      "Lizhi Wang",
      "Junjun Zhao",
      "Wending Zhao",
      "Feng Zhou",
      "Yonghao Dang",
      "Jianqin Yin"
    ],
    "abstract": "Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. While existing 2D and 3D physical attacks typically optimize texture, they often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module to preserve geometric fidelity, and a physical augmentation module to simulate complex physical scenarios, thus enhancing attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21% to 7.38%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks. These results validate 3DGAA as a practical attack framework for evaluating the safety of perception systems in autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2507.09993v1",
    "pdf_url": "http://arxiv.org/pdf/2507.09993v1",
    "published_date": "2025-07-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "ar",
      "autonomous driving",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning human-to-robot handovers through 3D scene reconstruction",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although training using simulations offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. Gaussian Splatting visual reconstruction methods have recently provided new directions for robot manipulation by generating realistic environments. In this paper, we propose the first method for learning supervised-based robot handovers solely from RGB images without the need of real-robot training or real-robot data collection. The proposed policy learner, Human-to-Robot Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. We train a robot policy on demonstrations collected with 16 household objects and {\\em directly} deploy this policy in the real environment. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that H2RH-SGS serves as a new and effective representation for the human-to-robot handover task.",
    "arxiv_url": "http://arxiv.org/abs/2507.08726v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08726v1",
    "published_date": "2025-07-11",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "sparse-view",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RePaintGS: Reference-Guided Gaussian Splatting for Realistic and\n  View-Consistent 3D Scene Inpainting",
    "authors": [
      "Ji Hyun Seo",
      "Byounhyun Yoo",
      "Gerard Jounghyun Kim"
    ],
    "abstract": "Radiance field methods, such as Neural Radiance Field or 3D Gaussian Splatting, have emerged as seminal 3D representations for synthesizing realistic novel views. For practical applications, there is ongoing research on flexible scene editing techniques, among which object removal is a representative task. However, removing objects exposes occluded regions, often leading to unnatural appearances. Thus, studies have employed image inpainting techniques to replace such regions with plausible content - a task referred to as 3D scene inpainting. However, image inpainting methods produce one of many plausible completions for each view, leading to inconsistencies between viewpoints. A widely adopted approach leverages perceptual cues to blend inpainted views smoothly. However, it is prone to detail loss and can fail when there are perceptual inconsistencies across views. In this paper, we propose a novel 3D scene inpainting method that reliably produces realistic and perceptually consistent results even for complex scenes by leveraging a reference view. Given the inpainted reference view, we estimate the inpainting similarity of the other views to adjust their contribution in constructing an accurate geometry tailored to the reference. This geometry is then used to warp the reference inpainting to other views as pseudo-ground truth, guiding the optimization to match the reference appearance. Comparative evaluation studies have shown that our approach improves both the geometric fidelity and appearance consistency of inpainted scenes.",
    "arxiv_url": "http://arxiv.org/abs/2507.08434v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08434v1",
    "published_date": "2025-07-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Temporally Consistent Amodal Completion for 3D Human-Object Interaction\n  Reconstruction",
    "authors": [
      "Hyungjun Doh",
      "Dong In Lee",
      "Seunggeun Chi",
      "Pin-Hao Huang",
      "Kwonjoon Lee",
      "Sangpil Kim",
      "Karthik Ramani"
    ],
    "abstract": "We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.",
    "arxiv_url": "http://arxiv.org/abs/2507.08137v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08137v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "3d gaussian",
      "3d reconstruction",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration",
    "authors": [
      "Chong Cheng",
      "Yu Hu",
      "Sicheng Yu",
      "Beizhen Zhao",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.",
    "arxiv_url": "http://arxiv.org/abs/2507.08136v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08136v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "sparse view",
      "geometry",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance\n  Transfer and Reflection",
    "authors": [
      "Yongyang Zhou",
      "Fang-Lue Zhang",
      "Zichen Wang",
      "Lei Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.",
    "arxiv_url": "http://arxiv.org/abs/2507.07733v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07733v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "relighting",
      "lighting",
      "reflection",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A\n  Benchmark for 3D Segmentation",
    "authors": [
      "Bangning Wei",
      "Joshua Maraval",
      "Meriem Outtas",
      "Kidiyo Kpalma",
      "Nicolas Ramin",
      "Lu Zhang"
    ],
    "abstract": "The application of methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D object segmentation in static scenes. These approaches demonstrate efficacy in a range of 3D scene understanding and editing tasks. Nevertheless, the 4D object segmentation of dynamic scenes remains an underexplored field due to the absence of a sufficiently extensive and accurately labelled multi-view video dataset. In this paper, we present MUVOD, a new multi-view video dataset for training and evaluating object segmentation in reconstructed real-world scenarios. The 17 selected scenes, describing various indoor or outdoor activities, are collected from different sources of datasets originating from various types of camera rigs. Each scene contains a minimum of 9 views and a maximum of 46 views. We provide 7830 RGB images (30 frames per video) with their corresponding segmentation mask in 4D motion, meaning that any object of interest in the scene could be tracked across temporal frames of a given view or across different views belonging to the same camera rig. This dataset, which contains 459 instances of 73 categories, is intended as a basic benchmark for the evaluation of multi-view video segmentation methods. We also present an evaluation metric and a baseline segmentation approach to encourage and evaluate progress in this evolving field. Additionally, we propose a new benchmark for 3D object segmentation task with a subset of annotated multi-view images selected from our MUVOD dataset. This subset contains 50 objects of different conditions in different scenarios, providing a more comprehensive analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.",
    "arxiv_url": "http://arxiv.org/abs/2507.07519v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07519v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "understanding",
      "segmentation",
      "nerf",
      "dynamic",
      "ar",
      "outdoor",
      "4d",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene\n  Reconstruction",
    "authors": [
      "Wei Yao",
      "Shuzhao Xie",
      "Letian Li",
      "Weixiang Zhang",
      "Zhixin Lai",
      "Shiqi Dai",
      "Ke Zhang",
      "Zhi Wang"
    ],
    "abstract": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver impressive visual fidelity and rendering speed, however, the inherent trade-off between storage costs and the ability to characterize complex physical motions significantly limits the practical application of these methods. To tackle these problems, we propose SD-GS, a compact and efficient dynamic Gaussian splatting framework for complex dynamic scene reconstruction, featuring two key contributions. First, we introduce a deformable anchor grid, a hierarchical and memory-efficient scene representation where each anchor point derives multiple 3D Gaussians in its local spatiotemporal region and serves as the geometric backbone of the 3D scene. Second, to enhance modeling capability for complex motions, we present a deformation-aware densification strategy that adaptively grows anchors in under-reconstructed high-dynamic regions while reducing redundancy in static areas, achieving superior visual quality with fewer anchors. Experimental results demonstrate that, compared to state-of-the-art methods, SD-GS achieves an average of 60\\% reduction in model size and an average of 100\\% improvement in FPS, significantly enhancing computational efficiency while maintaining or even surpassing visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2507.07465v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07465v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "dynamic",
      "ar",
      "deformation",
      "4d",
      "gaussian splatting",
      "motion",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for\n  Unconstrained Image Collections",
    "authors": [
      "Yongtang Bao",
      "Chengjie Tang",
      "Yuze Wang",
      "Haojie Li"
    ],
    "abstract": "Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.",
    "arxiv_url": "http://arxiv.org/abs/2507.07395v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07395v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "segmentation",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian\n  Splatting",
    "authors": [
      "Wijayathunga W. M. R. D. B"
    ],
    "abstract": "We propose a novel framework that enhances non-rigid 3D model deformations by bridging mesh representations with 3D Gaussian splatting. While traditional Gaussian splatting delivers fast, real-time radiance-field rendering, its post-editing capabilities and support for large-scale, non-rigid deformations remain limited. Our method addresses these challenges by embedding Gaussian kernels directly onto explicit mesh surfaces. This allows the mesh's inherent topological and geometric priors to guide intuitive editing operations -- such as moving, scaling, and rotating individual 3D components -- and enables complex deformations like bending and stretching. This work paves the way for more flexible 3D content-creation workflows in applications spanning virtual reality, character animation, and interactive design.",
    "arxiv_url": "http://arxiv.org/abs/2507.07000v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07000v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "fast",
      "3d gaussian",
      "face",
      "ar",
      "deformation",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photometric Stereo using Gaussian Splatting and inverse rendering",
    "authors": [
      "Matéo Ducastel",
      "David Tschumperlé",
      "Yvain Quéau"
    ],
    "abstract": "Recent state-of-the-art algorithms in photometric stereo rely on neural networks and operate either through prior learning or inverse rendering optimization. Here, we revisit the problem of calibrated photometric stereo by leveraging recent advances in 3D inverse rendering using the Gaussian Splatting formalism. This allows us to parameterize the 3D scene to be reconstructed and optimize it in a more interpretable manner. Our approach incorporates a simplified model for light representation and demonstrates the potential of the Gaussian Splatting rendering engine for the photometric stereo problem.",
    "arxiv_url": "http://arxiv.org/abs/2507.06684v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06684v1",
    "published_date": "2025-07-09",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for\n  3D Gaussian Splatting",
    "authors": [
      "Boyuan Tian",
      "Qizhe Gao",
      "Siran Xianyu",
      "Xiaotong Cui",
      "Minjia Zhang"
    ],
    "abstract": "3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (<1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: https://github.com/Supercomputing-System-AI-Lab/FlexGaussian",
    "arxiv_url": "http://arxiv.org/abs/2507.06671v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06671v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic\n  Visualization of Volumetric Medical Data",
    "authors": [
      "Chengkun Li",
      "Yuqi Tong",
      "Kai Chen",
      "Zhenya Yang",
      "Ruiyang Li",
      "Shi Qiu",
      "Jason Ying-Kuen Chan",
      "Pheng-Ann Heng",
      "Qi Dou"
    ],
    "abstract": "The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2507.06647v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06647v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "understanding",
      "dynamic",
      "ar",
      "deformation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
    "authors": [
      "Wanhua Li",
      "Yujie Zhao",
      "Minghan Qin",
      "Yang Liu",
      "Yuanhao Cai",
      "Chuang Gan",
      "Hanspeter Pfister"
    ],
    "abstract": "In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\\times$ speedup and a 47 $\\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2507.07136v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07136v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "ar",
      "high quality",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for\n  Panorama-Style Mobile Captures",
    "authors": [
      "Seungoh Han",
      "Jaehoon Jang",
      "Hyunsu Kim",
      "Jaeheung Surh",
      "Junhyung Kwak",
      "Hyowon Ha",
      "Kyungdon Joo"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement.",
    "arxiv_url": "http://arxiv.org/abs/2507.06109v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06109v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "ar",
      "gaussian splatting",
      "high-fidelity",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D\n  Gaussian Splatting for Photorealistic Scenes Rendering",
    "authors": [
      "Jiayi Song",
      "Zihan Ye",
      "Qingyuan Zhou",
      "Weidong Yang",
      "Ben Fei",
      "Jingyi Xu",
      "Ying He",
      "Wanli Ouyang"
    ],
    "abstract": "Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2507.06103v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06103v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "geometry",
      "reflection",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis",
    "authors": [
      "Alexandre Symeonidis-Herzig",
      "Özge Mercanoğlu Sincan",
      "Richard Bowden"
    ],
    "abstract": "Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.",
    "arxiv_url": "http://arxiv.org/abs/2507.06060v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06060v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "human",
      "3d gaussian",
      "avatar",
      "ar",
      "recognition",
      "animation",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for\n  Free-Viewpoint Videos",
    "authors": [
      "Wenkang Zhang",
      "Yan Zhao",
      "Qiang Wang",
      "Li Song",
      "Zhengxue Cheng"
    ],
    "abstract": "Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representations remains a major challenge. Recent advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have enabled high-fidelity scene modeling. However, existing methods often couple scene reconstruction with optimization-dependent coding, which limits generalizability. This paper presents Feedforward Compression of Dynamic Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing temporally correlated Gaussian point cloud sequences. Our approach introduces a Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame motions are extracted via sparse control points. The resulting motion tensors are compressed in a feedforward manner using a dual prior-aware entropy model that combines hyperprior and spatial-temporal priors for accurate rate estimation. For reconstruction, we perform control-point-guided motion compensation and employ a refinement network to enhance view-consistent fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS generalizes across scenes without per-scene optimization. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression in under 2 seconds while preserving visual quality across viewpoints. This work advances feedforward compression for dynamic 3DGS, paving the way for scalable FVV transmission and storage in immersive applications.",
    "arxiv_url": "http://arxiv.org/abs/2507.05859v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05859v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "efficient",
      "ar",
      "compression",
      "gaussian splatting",
      "high-fidelity",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamArt: Generating Interactable Articulated Objects from a Single\n  Image",
    "authors": [
      "Ruijie Lu",
      "Yu Liu",
      "Jiaxiang Tang",
      "Junfeng Ni",
      "Yuxiang Wang",
      "Diwen Wan",
      "Gang Zeng",
      "Yixin Chen",
      "Siyuan Huang"
    ],
    "abstract": "Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.",
    "arxiv_url": "http://arxiv.org/abs/2507.05763v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05763v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "face",
      "geometry",
      "segmentation",
      "ar",
      "nerf",
      "gaussian splatting",
      "high-fidelity",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D\n  Gaussian Splatting",
    "authors": [
      "Haitao Lu",
      "Haijier Chen",
      "Haoze Liu",
      "Shoujian Zhang",
      "Bo Xu",
      "Ziao Liu"
    ],
    "abstract": "In autonomous robotic systems, precise localization is a prerequisite for safe navigation. However, in complex urban environments, GNSS positioning often suffers from signal occlusion and multipath effects, leading to unreliable absolute positioning. Traditional mapping approaches are constrained by storage requirements and computational inefficiency, limiting their applicability to resource-constrained robotic platforms. To address these challenges, we propose 3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian Splatting (3DGS), enabling centimeter-level positioning using only a single monocular RGB image on the client side. We combine multi-sensor data to construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side localization requires just a standard camera input. Using SuperPoint and SuperGlue for feature extraction and matching, our core innovation is an iterative optimization strategy that refines localization results through step-by-step rendering, making it suitable for real-time autonomous navigation. Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, significantly outperforming other representative methods while requiring only monocular RGB input. This approach provides autonomous robots with reliable localization capabilities even in challenging urban environments where GNSS fails.",
    "arxiv_url": "http://arxiv.org/abs/2507.05661v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05661v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving",
      "ar",
      "outdoor",
      "localization",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry\n  with Neural Signed Distance Fields",
    "authors": [
      "Rushil Desai"
    ],
    "abstract": "Quantifying uncertainty in neural implicit 3D representations, particularly those utilizing Signed Distance Functions (SDFs), remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. Existing methods typically neglect direct geometric integration, leading to poorly calibrated uncertainty maps. We introduce BayesSDF, a novel probabilistic framework for uncertainty quantification in neural implicit SDF models, motivated by scientific simulation applications with 3D environments (e.g., forests) such as modeling fluid flow through forests, where precise surface geometry and reliable uncertainty estimates are essential. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian splatting, which lack explicit surface formulations, Signed Distance Functions (SDFs) define continuous and differentiable geometry, making them better suited for physical modeling and analysis. BayesSDF leverages a Laplace approximation to quantify local surface instability using Hessian-based metrics, enabling efficient, surfaceaware uncertainty estimation. Our method shows that uncertainty predictions correspond closely with poorly reconstructed geometry, providing actionable confidence measures for downstream use. Extensive evaluations on synthetic and real-world datasets demonstrate that BayesSDF outperforms existing methods in both calibration and geometric consistency, establishing a strong foundation for uncertainty-aware 3D scene reconstruction, simulation, and robotic decision-making.",
    "arxiv_url": "http://arxiv.org/abs/2507.06269v2",
    "pdf_url": "http://arxiv.org/pdf/2507.06269v2",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mastering Regional 3DGS: Locating, Initializing, and Editing with\n  Diverse 2D Priors",
    "authors": [
      "Lanqing Guo",
      "Yufei Wang",
      "Hezhen Hu",
      "Yan Zheng",
      "Yeying Jin",
      "Siyu Huang",
      "Zhangyang Wang"
    ],
    "abstract": "Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\\times$ speedup, providing a more efficient and effective approach to 3D scene local editing.",
    "arxiv_url": "http://arxiv.org/abs/2507.05426v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05426v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d gaussian",
      "semantic",
      "ar",
      "localization",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with\n  Segmented Consistency Trajectory Distillation",
    "authors": [
      "Jiahao Zhu",
      "Zixuan Chen",
      "Guangcong Wang",
      "Xiaohua Xie",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).",
    "arxiv_url": "http://arxiv.org/abs/2507.05256v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05256v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D\n  Geometry-Consistent Attention Prior",
    "authors": [
      "Minghao Wen",
      "Shengjie Wu",
      "Kangkan Wang",
      "Dong Liang"
    ],
    "abstract": "3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a \"one-shot deal\", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.",
    "arxiv_url": "http://arxiv.org/abs/2507.04961v1",
    "pdf_url": "http://arxiv.org/pdf/2507.04961v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "semantic",
      "ar",
      "deformation",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated\n  Rendering in Virtual Reality",
    "authors": [
      "Shuo Xin",
      "Haiyu Wang",
      "Sai Qian Zhang"
    ],
    "abstract": "Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process.   Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\\times$ while maintaining visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2507.04147v1",
    "pdf_url": "http://arxiv.org/pdf/2507.04147v1",
    "published_date": "2025-07-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "3d gaussian",
      "tracking",
      "vr",
      "face",
      "neural rendering",
      "dynamic",
      "efficient rendering",
      "ar",
      "head",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM",
    "authors": [
      "Xiaolei Lang",
      "Jiajun Lv",
      "Kai Tang",
      "Laijian Li",
      "Jianxin Huang",
      "Lina Liu",
      "Yong Liu",
      "Xingxing Zuo"
    ],
    "abstract": "This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian Splatting SLAM system that simultaneously addresses visual quality, geometric accuracy, and real-time performance. The proposed method performs robust and accurate pose estimation within a continuous-time trajectory optimization framework, while incrementally reconstructing a 3D Gaussian map using camera and LiDAR data, all in real time. The resulting map enables high-quality, real-time novel view rendering of both RGB images and depth maps. To effectively address under-reconstruction in regions not covered by the LiDAR, we employ a lightweight zero-shot depth model that synergistically combines RGB appearance cues with sparse LiDAR measurements to generate dense depth maps. The depth completion enables reliable Gaussian initialization in LiDAR-blind areas, significantly improving system applicability for sparse LiDAR sensors. To enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise Gaussian map optimization and accelerate it with carefully designed CUDA-accelerated strategies. Furthermore, we explore how the incrementally reconstructed Gaussian map can improve the robustness of odometry. By tightly incorporating photometric constraints from the Gaussian map into the continuous-time factor graph optimization, we demonstrate improved pose estimation under LiDAR degradation scenarios. We also showcase downstream applications via extending our elaborate system, including video frame interpolation and fast 3D mesh extraction. To support rigorous evaluation, we construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth poses, depth maps, and extrapolated trajectories for assessing out-of-sequence novel view synthesis. Both the dataset and code will be made publicly available on project page https://xingxingzuo.github.io/gaussian_lic2.",
    "arxiv_url": "http://arxiv.org/abs/2507.04004v2",
    "pdf_url": "http://arxiv.org/pdf/2507.04004v2",
    "published_date": "2025-07-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "slam",
      "ar",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic\n  Urban Environments",
    "authors": [
      "Guile Wu",
      "Dongfeng Bai",
      "Bingbing Liu"
    ],
    "abstract": "This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2507.03886v1",
    "pdf_url": "http://arxiv.org/pdf/2507.03886v1",
    "published_date": "2025-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d gaussian",
      "urban scene",
      "dynamic",
      "autonomous driving",
      "ar",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian\n  Pointmaps",
    "authors": [
      "Chong Cheng",
      "Sicheng Yu",
      "Zijian Wang",
      "Yifan Zhou",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, \\textbf{lack geometric priors} in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to \\textbf{scale drift}. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.",
    "arxiv_url": "http://arxiv.org/abs/2507.03737v1",
    "pdf_url": "http://arxiv.org/pdf/2507.03737v1",
    "published_date": "2025-07-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "tracking",
      "slam",
      "dynamic",
      "ar",
      "outdoor",
      "gaussian splatting",
      "high-fidelity",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars",
    "authors": [
      "Gent Serifi",
      "Marcel C. Bühler"
    ],
    "abstract": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.",
    "arxiv_url": "http://arxiv.org/abs/2507.02803v2",
    "pdf_url": "http://arxiv.org/pdf/2507.02803v2",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "lighting",
      "face",
      "avatar",
      "reflection",
      "ar",
      "deformation",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and\n  Manipulation of Articulated Objects",
    "authors": [
      "Qiaojun Yu",
      "Xibin Yuan",
      "Yu jiang",
      "Junting Chen",
      "Dongzhe Zheng",
      "Ce Hao",
      "Yang You",
      "Yixing Chen",
      "Yao Mu",
      "Liu Liu",
      "Cewu Lu"
    ],
    "abstract": "Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: https://sites.google.com/view/artgs/home",
    "arxiv_url": "http://arxiv.org/abs/2507.02600v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02600v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "robotics",
      "understanding",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local\n  Implicit Feature Decoupling",
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Jianbo Jiao",
      "Jiayu Yang",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jie Liang",
      "Jinbo Yan",
      "Runling Liu",
      "Ronggang Wang"
    ],
    "abstract": "Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.",
    "arxiv_url": "http://arxiv.org/abs/2507.02363v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02363v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes",
    "authors": [
      "Stephen Pasch",
      "Joel K. Salzman",
      "Changxi Zheng"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.",
    "arxiv_url": "http://arxiv.org/abs/2507.02257v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02257v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "geometry",
      "reflection",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial\n  Camouflage Generation",
    "authors": [
      "Tianrui Lou",
      "Xiaojun Jia",
      "Siyuan Liang",
      "Jiawei Liang",
      "Ming Zhang",
      "Yanjun Xiao",
      "Xiaochun Cao"
    ],
    "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:https://github.com/TRLou/PGA.",
    "arxiv_url": "http://arxiv.org/abs/2507.01367v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01367v1",
    "published_date": "2025-07-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d gaussian",
      "autonomous driving",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online\n  Semantic Gaussian Splatting",
    "authors": [
      "Keiko Nagami",
      "Timothy Chen",
      "Javier Yu",
      "Ola Shorinwa",
      "Maximilian Adang",
      "Carlyn Dougherty",
      "Eric Cristofalo",
      "Mac Schwager"
    ],
    "abstract": "We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., \"find a person\"), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2507.01125v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01125v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
    "authors": [
      "Felix Windisch",
      "Lukas Radl",
      "Thomas Köhler",
      "Michael Steiner",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
    "arxiv_url": "http://arxiv.org/abs/2507.01110v2",
    "pdf_url": "http://arxiv.org/pdf/2507.01110v2",
    "published_date": "2025-07-01",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "vr",
      "dynamic",
      "lightweight",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Masks make discriminative models great again!",
    "authors": [
      "Tianshi Cao",
      "Marie-Julie Rakotosaona",
      "Ben Poole",
      "Federico Tombari",
      "Michael Niemeyer"
    ],
    "abstract": "We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques.",
    "arxiv_url": "http://arxiv.org/abs/2507.00916v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00916v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using\n  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond",
    "authors": [
      "Anna-Maria Halacheva",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.",
    "arxiv_url": "http://arxiv.org/abs/2507.00886v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00886v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "understanding",
      "compact",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail\n  Conserved Anti-Aliasing",
    "authors": [
      "Zhenya Yang",
      "Bingchen Gong",
      "Kai Chen"
    ],
    "abstract": "Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.",
    "arxiv_url": "http://arxiv.org/abs/2507.00554v2",
    "pdf_url": "http://arxiv.org/pdf/2507.00554v2",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d gaussian",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And\n  Dynamic Density Control",
    "authors": [
      "Xingjun Wang",
      "Lianlei Shan"
    ],
    "abstract": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time.",
    "arxiv_url": "http://arxiv.org/abs/2507.00363v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00363v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "fast",
      "3d gaussian",
      "face",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction",
    "authors": [
      "Antoine Guédon",
      "Diego Gomez",
      "Nissim Maruani",
      "Bingchen Gong",
      "George Drettakis",
      "Maks Ovsjanikov"
    ],
    "abstract": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
    "arxiv_url": "http://arxiv.org/abs/2506.24096v1",
    "pdf_url": "http://arxiv.org/pdf/2506.24096v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "fast",
      "3d gaussian",
      "face",
      "geometry",
      "ar",
      "animation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local\n  Reconstruction and Rendering",
    "authors": [
      "Zinuo You",
      "Stamatios Georgoulis",
      "Anpei Chen",
      "Siyu Tang",
      "Dengxin Dai"
    ],
    "abstract": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \\textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.",
    "arxiv_url": "http://arxiv.org/abs/2506.23957v2",
    "pdf_url": "http://arxiv.org/pdf/2506.23957v2",
    "published_date": "2025-06-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via\n  Structural Attention",
    "authors": [
      "Ziao Liu",
      "Zhenjia Li",
      "Yifeng Shi",
      "Xiangang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2506.23611v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23611v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "face",
      "efficient rendering",
      "ar",
      "nerf",
      "gaussian splatting",
      "motion",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image\n  Representation via 2D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Chao Yang",
      "Tao Guan",
      "Lili Ju"
    ],
    "abstract": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2506.23479v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23479v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable\n  Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Kun Yuan",
      "Guankun Wang",
      "Mobarak I. Hoque",
      "Nicolas Padoy",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "abstract": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.23309v2",
    "pdf_url": "http://arxiv.org/pdf/2506.23309v2",
    "published_date": "2025-06-29",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "tracking",
      "3d reconstruction",
      "understanding",
      "segmentation",
      "semantic",
      "ar",
      "deformation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination\n  Correction with Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Yanheng Li",
      "Tong Chen",
      "Jie Wang",
      "Jinlin Wu",
      "Zhen Lei",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
    "arxiv_url": "http://arxiv.org/abs/2506.23308v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23308v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d gaussian",
      "lighting",
      "dynamic",
      "ar",
      "4d",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric\n  Constraints",
    "authors": [
      "Zhen Tan",
      "Xieyuanli Chen",
      "Lei Feng",
      "Yangbing Ge",
      "Shuaifeng Zhi",
      "Jiaxiong Liu",
      "Dewen Hu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.",
    "arxiv_url": "http://arxiv.org/abs/2506.23207v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23207v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d gaussian",
      "lighting",
      "tracking",
      "geometry",
      "slam",
      "dynamic",
      "ar",
      "outdoor",
      "illumination",
      "high-fidelity",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STD-GS: Exploring Frame-Event Interaction for\n  SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic\n  Scene",
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Luxin Yan",
      "Gim Hee Lee"
    ],
    "abstract": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2506.23157v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23157v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient\n  3D Gaussian Splatting",
    "authors": [
      "Hung Nguyen",
      "An Le",
      "Runfa Li",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.",
    "arxiv_url": "http://arxiv.org/abs/2506.23042v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23042v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions",
    "authors": [
      "AmirHossein Naghi Razlighi",
      "Elaheh Badali Golezani",
      "Shohreh Kasaei"
    ],
    "abstract": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting",
    "arxiv_url": "http://arxiv.org/abs/2506.22973v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22973v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d gaussian",
      "ar",
      "head",
      "gaussian splatting",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via\n  Diffusion Priors",
    "authors": [
      "Sicong Du",
      "Jiarun Liu",
      "Qifeng Chen",
      "Hao-Xiang Chen",
      "Tai-Jiang Mu",
      "Sheng Yang"
    ],
    "abstract": "A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS.",
    "arxiv_url": "http://arxiv.org/abs/2506.22800v2",
    "pdf_url": "http://arxiv.org/pdf/2506.22800v2",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding",
    "authors": [
      "Minchao Jiang",
      "Shunyu Jia",
      "Jiaming Gu",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Anqi Dong",
      "Liang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time rendering for novel view synthesis of 3D scenes. However, existing methods focus primarily on geometric and appearance modeling, lacking deeper scene understanding while also incurring high training costs that complicate the originally streamlined differentiable rendering pipeline. To this end, we propose VoteSplat, a novel 3D scene understanding framework that integrates Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized for instance segmentation, extracting objects, and generating 2D vote maps. We then embed spatial offset vectors into Gaussian primitives. These offsets construct 3D spatial votes by associating them with 2D image votes, while depth distortion constraints refine localization along the depth axis. For open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D point clouds via voting points, reducing training costs associated with high-dimensional CLIP features while preserving semantic unambiguity. Extensive experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and ablation studies. Our code is available at https://sy-ja.github.io/votesplat/",
    "arxiv_url": "http://arxiv.org/abs/2506.22799v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22799v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d gaussian",
      "localization",
      "understanding",
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
    "authors": [
      "Tao Tang",
      "Likui Zhang",
      "Youpeng Wen",
      "Kaidong Zhang",
      "Jia-Wang Bian",
      "xia zhou",
      "Tianyi Yan",
      "Kun Zhan",
      "Peng Jia",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "abstract": "The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.22756v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22756v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "ar",
      "4d",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian\n  Splatting and a Low-Rank Free-Form Deformation Model",
    "authors": [
      "Yuliang Huang",
      "Imraj Singh",
      "Thomas Joyce",
      "Kris Thielemans",
      "Jamie R. McClelland"
    ],
    "abstract": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.22280v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22280v1",
    "published_date": "2025-06-27",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "deformation",
      "4d",
      "gaussian splatting",
      "motion",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve\n  Gaussian Splatting",
    "authors": [
      "Zipei Ma",
      "Junzhe Jiang",
      "Yurui Chen",
      "Li Zhang"
    ],
    "abstract": "The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\\'ezier curve Gaussian splatting (B\\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.22099v3",
    "pdf_url": "http://arxiv.org/pdf/2506.22099v3",
    "published_date": "2025-06-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "urban scene",
      "autonomous driving",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MADrive: Memory-Augmented Driving Scene Modeling",
    "authors": [
      "Polina Karpikova",
      "Daniil Selikhanovych",
      "Kirill Struminsky",
      "Ruslan Musaev",
      "Maria Golitsyna",
      "Dmitry Baranchuk"
    ],
    "abstract": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/",
    "arxiv_url": "http://arxiv.org/abs/2506.21520v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21520v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "relighting",
      "lighting",
      "autonomous driving",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian\n  Splatting",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Zhuoxiao Li",
      "Haocheng Zhao",
      "Kang Dang",
      "Jionglong Su",
      "Limin Yu",
      "Haoang Li"
    ],
    "abstract": "Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.21420v2",
    "pdf_url": "http://arxiv.org/pdf/2506.21420v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "face",
      "localization",
      "slam",
      "mapping",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image",
    "authors": [
      "Pufan Li",
      "Bi'an Du",
      "Wei Hu"
    ],
    "abstract": "Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.",
    "arxiv_url": "http://arxiv.org/abs/2506.21152v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21152v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "68",
      "I.4.0"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local\n  Optimization",
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Songyou Peng"
    ],
    "abstract": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.21117v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21117v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "segmentation",
      "dynamic",
      "ar",
      "head",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "User-in-the-Loop View Sampling with Error Peaking Visualization",
    "authors": [
      "Ayaka Yasunaga",
      "Hideo Saito",
      "Shohei Mori"
    ],
    "abstract": "Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2506.21009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21009v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via\n  Sparse-Controlled Gaussian Splatting",
    "authors": [
      "Yeon-Ji Song",
      "Jaein Kim",
      "Byung-Ju Kim",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.",
    "arxiv_url": "http://arxiv.org/abs/2506.20998v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20998v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGH: 3D Head Generation with Composable Hair and Face",
    "authors": [
      "Chengan He",
      "Junxuan Li",
      "Tobias Kirschstein",
      "Artem Sevastopolsky",
      "Shunsuke Saito",
      "Qingyang Tan",
      "Javier Romero",
      "Chen Cao",
      "Holly Rushmeier",
      "Giljoo Nam"
    ],
    "abstract": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.",
    "arxiv_url": "http://arxiv.org/abs/2506.20875v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20875v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "3d gaussian",
      "face",
      "geometry",
      "ar",
      "head",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and\n  Rasterizer",
    "authors": [
      "Da Li",
      "Donggang Jia",
      "Yousef Rajeh",
      "Dominik Engel",
      "Ivan Viola"
    ],
    "abstract": "With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.",
    "arxiv_url": "http://arxiv.org/abs/2506.20202v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20202v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "ray tracing",
      "ar",
      "gaussian splatting",
      "high-fidelity",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target\n  Reconstruction",
    "authors": [
      "Aobo Li",
      "Zhengxin Lei",
      "Jiangtao Wei",
      "Feng Xu"
    ],
    "abstract": "Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.",
    "arxiv_url": "http://arxiv.org/abs/2506.21633v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21633v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded\n  Scenes",
    "authors": [
      "Chenhao Zhang",
      "Yezhi Shen",
      "Fengqing Zhu"
    ],
    "abstract": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.21629v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21629v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "neural rendering",
      "ar",
      "outdoor",
      "nerf",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical\n  Gaussian World Model",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.",
    "arxiv_url": "http://arxiv.org/abs/2506.19842v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19842v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "dynamic",
      "ar",
      "deformation",
      "body",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtual Memory for 3D Gaussian Splatting",
    "authors": [
      "Jonathan Haberl",
      "Philipp Fleck",
      "Clemens Arth"
    ],
    "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.",
    "arxiv_url": "http://arxiv.org/abs/2506.19415v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19415v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d gaussian",
      "dynamic",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis",
    "authors": [
      "Xiaoyuan Wang",
      "Yizhou Zhao",
      "Botao Ye",
      "Xiaojun Shan",
      "Weijie Lyu",
      "Lu Qi",
      "Kelvin C. K. Chan",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \\ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2506.19291v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19291v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "dynamic",
      "ar",
      "deformation",
      "head",
      "4d",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale\n  Multi-Agent Gaussian SLAM",
    "authors": [
      "Annika Thomas",
      "Aneesa Sonawalla",
      "Alex Rose",
      "Jonathan P. How"
    ],
    "abstract": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.",
    "arxiv_url": "http://arxiv.org/abs/2506.18885v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18885v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "tracking",
      "slam",
      "ar",
      "outdoor",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "authors": [
      "Michal Nazarczuk",
      "Sibi Catley-Chandar",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Gregory Slabaugh",
      "Eduardo Pérez-Pellitero"
    ],
    "abstract": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io",
    "arxiv_url": "http://arxiv.org/abs/2506.18792v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18792v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "ar",
      "4d",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Arena: An Open Platform for Generative 3D Evaluation",
    "authors": [
      "Dylan Ebert"
    ],
    "abstract": "Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons.   Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource.   Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D.",
    "arxiv_url": "http://arxiv.org/abs/2506.18787v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18787v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting",
    "authors": [
      "Adam Yang",
      "Nadula Kadawedduwa",
      "Tianfu Wang",
      "Maria Molina",
      "Christopher Metzler"
    ],
    "abstract": "Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.18677v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18677v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in\n  Large-Scale Scene",
    "authors": [
      "Shihan Chen",
      "Zhaojin Li",
      "Zeyu Chen",
      "Qingsong Yan",
      "Gaoyang Shen",
      "Ran Duan"
    ],
    "abstract": "Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2506.17636v1",
    "pdf_url": "http://arxiv.org/pdf/2506.17636v1",
    "published_date": "2025-06-21",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "survey",
      "dynamic",
      "autonomous driving",
      "ar",
      "outdoor",
      "nerf",
      "gaussian splatting",
      "high-fidelity",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement\n  for 3D Low-Level Vision",
    "authors": [
      "Weeyoung Kwon",
      "Jeahun Sung",
      "Minkyu Jeon",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "abstract": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2506.16262v2",
    "pdf_url": "http://arxiv.org/pdf/2506.16262v2",
    "published_date": "2025-06-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "robotics",
      "3d reconstruction",
      "neural rendering",
      "survey",
      "autonomous driving",
      "ar",
      "nerf",
      "gaussian splatting",
      "high-fidelity",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Information-computation trade-offs in non-linear transforms",
    "authors": [
      "Connor Ding",
      "Abhiram Rao Gorle",
      "Jiwon Jeong",
      "Naomi Sagan",
      "Tsachy Weissman"
    ],
    "abstract": "In this work, we explore the interplay between information and computation in non-linear transform-based compression for broad classes of modern information-processing tasks. We first investigate two emerging nonlinear data transformation frameworks for image compression: Implicit Neural Representations (INRs) and 2D Gaussian Splatting (GS). We analyze their representational properties, behavior under lossy compression, and convergence dynamics. Our results highlight key trade-offs between INR's compact, resolution-flexible neural field representations and GS's highly parallelizable, spatially interpretable fitting, providing insights for future hybrid and compression-aware frameworks. Next, we introduce the textual transform that enables efficient compression at ultra-low bitrate regimes and simultaneously enhances human perceptual satisfaction. When combined with the concept of denoising via lossy compression, the textual transform becomes a powerful tool for denoising tasks. Finally, we present a Lempel-Ziv (LZ78) \"transform\", a universal method that, when applied to any member of a broad compressor family, produces new compressors that retain the asymptotic universality guarantees of the LZ78 algorithm. Collectively, these three transforms illuminate the fundamental trade-offs between coding efficiency and computational cost. We discuss how these insights extend beyond compression to tasks such as classification, denoising, and generative AI, suggesting new pathways for using non-linear transformations to balance resource constraints and performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.15948v1",
    "pdf_url": "http://arxiv.org/pdf/2506.15948v1",
    "published_date": "2025-06-19",
    "categories": [
      "cs.IT",
      "eess.IV",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "compression",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos",
    "authors": [
      "Kaifeng Zhang",
      "Baoyu Li",
      "Kris Hauser",
      "Yunzhu Li"
    ],
    "abstract": "Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd .",
    "arxiv_url": "http://arxiv.org/abs/2506.15680v1",
    "pdf_url": "http://arxiv.org/pdf/2506.15680v1",
    "published_date": "2025-06-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate\n  Camera Pose Estimation under Complex Trajectories",
    "authors": [
      "Qingsong Yan",
      "Qiang Wang",
      "Kaiyong Zhao",
      "Jie Chen",
      "Bo Li",
      "Xiaowen Chu",
      "Fei Deng"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged as powerful tools for 3D reconstruction and SLAM tasks. However, their performance depends heavily on accurate camera pose priors. Existing approaches attempt to address this issue by introducing external constraints but fall short of achieving satisfactory accuracy, particularly when camera trajectories are complex. In this paper, we propose a novel method, RA-NeRF, capable of predicting highly accurate camera poses even with complex camera trajectories. Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF with photometric consistency and incorporates flow-driven pose regulation to enhance robustness during initialization and localization. Additionally, RA-NeRF employs an implicit pose filter to capture the camera movement pattern and eliminate the noise for pose estimation. To validate our method, we conduct extensive experiments on the Tanks\\&Temple dataset for standard evaluation, as well as the NeRFBuster dataset, which presents challenging camera pose trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in both camera pose estimation and visual quality, demonstrating its effectiveness and robustness in scene reconstruction under complex pose trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2506.15242v2",
    "pdf_url": "http://arxiv.org/pdf/2506.15242v2",
    "published_date": "2025-06-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "localization",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads\n  Synthesis Using Gaussian Splatting",
    "authors": [
      "Ziqiao Peng",
      "Wentao Hu",
      "Junyuan Ma",
      "Xiangyu Zhu",
      "Xiaomei Zhang",
      "Hao Zhao",
      "Hui Tian",
      "Jun He",
      "Hongyan Liu",
      "Zhaoxin Fan"
    ],
    "abstract": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.",
    "arxiv_url": "http://arxiv.org/abs/2506.14742v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14742v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "ar",
      "head",
      "gaussian splatting",
      "high-fidelity",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D\n  Gaussian-Splatting",
    "authors": [
      "Yuke Xing",
      "Jiarui Wang",
      "Peizhi Niu",
      "Wenjie Huang",
      "Guangtao Zhai",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.",
    "arxiv_url": "http://arxiv.org/abs/2506.14642v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14642v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "human",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty\n  Maps for 3D Reconstruction",
    "authors": [
      "Zhengquan Zhang",
      "Feng Xu",
      "Mengmi Zhang"
    ],
    "abstract": "Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.",
    "arxiv_url": "http://arxiv.org/abs/2506.14856v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14856v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "neural rendering",
      "mapping",
      "ar",
      "lightweight",
      "head",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient\n  High-Resolution 3D Reconstruction",
    "authors": [
      "Changbai Li",
      "Haodong Zhu",
      "Hanlin Chen",
      "Juan Zhang",
      "Tongfei Chen",
      "Shuo Yang",
      "Shuwei Shao",
      "Wenhao Dong",
      "Baochang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.14229v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14229v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "face",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAF: Gaussian Action Field as a Dynamic World Model for Robotic\n  Manipulation",
    "authors": [
      "Ying Chai",
      "Litao Deng",
      "Ruizhi Shao",
      "Jiajun Zhang",
      "Liangjun Xing",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a Vision-to-4D-to-Action (V-4D-A) framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/",
    "arxiv_url": "http://arxiv.org/abs/2506.14135v2",
    "pdf_url": "http://arxiv.org/pdf/2506.14135v2",
    "published_date": "2025-06-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "dynamic",
      "ar",
      "4d",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with\n  Gaussian Radiance Fields and Differentiable Dynamics",
    "authors": [
      "Qianzhong Chen",
      "Naixiang Gao",
      "Suning Huang",
      "JunEn Low",
      "Timothy Chen",
      "Jiankai Sun",
      "Mac Schwager"
    ],
    "abstract": "Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.",
    "arxiv_url": "http://arxiv.org/abs/2506.14009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14009v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "dynamic",
      "lightweight",
      "ar",
      "head",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated\n  Human Images",
    "authors": [
      "Lingteng Qiu",
      "Peihao Li",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Weihao Yuan",
      "Siyu Zhu",
      "Xiaoguang Han",
      "Guanying Chen",
      "Zilong Dong"
    ],
    "abstract": "Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.",
    "arxiv_url": "http://arxiv.org/abs/2506.13766v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13766v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "3d gaussian",
      "geometry",
      "avatar",
      "ar",
      "high-fidelity",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for\n  Unconstrained Scene Reconstruction",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.",
    "arxiv_url": "http://arxiv.org/abs/2506.13516v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13516v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "illumination",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate\n  Radiance Fields",
    "authors": [
      "Jungeon Kim",
      "Geonsoo Park",
      "Seungyong Lee"
    ],
    "abstract": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.13508v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13508v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "ar",
      "outdoor",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian\n  Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas.",
    "arxiv_url": "http://arxiv.org/abs/2506.13348v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13348v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "face",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object\n  Reconstruction",
    "authors": [
      "Jinguang Tong",
      "Xuesong li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ],
    "abstract": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS",
    "arxiv_url": "http://arxiv.org/abs/2506.13110v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13110v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "fast",
      "3d gaussian",
      "relighting",
      "lighting",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
    "authors": [
      "Hyunjin Kim",
      "Haebeom Jung",
      "Jaesik Park"
    ],
    "abstract": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or the premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep Blending, show that our approach reduces the number of Gaussians needed, enhancing computational efficiency while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.",
    "arxiv_url": "http://arxiv.org/abs/2506.12945v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12945v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting",
    "authors": [
      "Mufan Liu",
      "Cixiao Zhang",
      "Qi Yang",
      "Yujie Cao",
      "Yiling Xu",
      "Yin Xu",
      "Shu Sun",
      "Mingzeng Dai",
      "Yunfeng Guan"
    ],
    "abstract": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.",
    "arxiv_url": "http://arxiv.org/abs/2506.12787v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12787v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "localization",
      "compact",
      "ar",
      "lightweight",
      "deformation",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient multi-view training for 3D Gaussian Splatting",
    "authors": [
      "Minhyuk Choi",
      "Injae Kim",
      "Hyunwoo J. Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize \"single-view\" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's \"multi-view\" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.",
    "arxiv_url": "http://arxiv.org/abs/2506.12727v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12727v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "ar",
      "head",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors",
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Jianmeng Liu",
      "Mingxiao Huo",
      "Pavel Tokmakov",
      "Katerina Fragkiadaki"
    ],
    "abstract": "We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2506.12716v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12716v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "3d gaussian",
      "dynamic",
      "ar",
      "4d",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]